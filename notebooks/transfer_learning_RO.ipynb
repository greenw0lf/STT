{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45ea3ef5",
      "metadata": {
        "id": "45ea3ef5"
      },
      "source": [
        "# Transfer learn from English to Romanian STT model\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Setup the Romanian audio and metadata files.\n",
        "2. Download a pre-trained English STT model.\n",
        "3. Fine-tune the English model to Romanian language.\n",
        "4. Test the new Romanian model and display its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fa2aec77",
      "metadata": {
        "id": "fa2aec77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039e255d-78dc-4d7e-f51b-116bb9edc791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopus-dev is already the newest version (1.1.2-1ubuntu1).\n",
            "libopusfile-dev is already the newest version (0.9+20170913-1build1).\n",
            "libopusfile0 is already the newest version (0.9+20170913-1build1).\n",
            "libsox-fmt-mp3 is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: coqui_stt_training in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: coqui-stt-ctcdecoder==1.4.0 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.4.0)\n",
            "Requirement already satisfied: miniaudio in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.53)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (4.64.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (3.17.3)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.4.1)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.0.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.0.1)\n",
            "Requirement already satisfied: opuslib==2.0.0 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.0.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (3.0.3)\n",
            "Collecting tensorflow==1.15.4\n",
            "  Using cached tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "Requirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.53.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.23.0)\n",
            "Requirement already satisfied: resampy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.4.2)\n",
            "Requirement already satisfied: webdataset==0.1.103 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.1.103)\n",
            "Requirement already satisfied: clearml in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.15.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.18.5)\n",
            "Requirement already satisfied: coqpit in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.0.16)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.13.0)\n",
            "Requirement already satisfied: pyogg>=0.6.14a1 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.6.14a1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (3.38.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.14.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (2.0.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.50.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.2.2)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.103->coqui_stt_training) (0.1.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.103->coqui_stt_training) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (0.36.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui_stt_training) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (3.0.9)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (5.4.8)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (7.1.2)\n",
            "Requirement already satisfied: pyjwt<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (2.4.0)\n",
            "Requirement already satisfied: pathlib2>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (2.3.7.post1)\n",
            "Requirement already satisfied: furl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (2.1.3)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (1.24.3)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (4.3.3)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.7/dist-packages (from clearml->coqui_stt_training) (22.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from miniaudio->coqui_stt_training) (1.15.1)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (4.13.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (1.4.42)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (6.7.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (3.10.1)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (1.7.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (1.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui_stt_training) (2022.5)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui_stt_training) (3.3.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna->coqui_stt_training) (1.2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna->coqui_stt_training) (5.10.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12.0->miniaudio->coqui_stt_training) (2.21)\n",
            "Requirement already satisfied: orderedmultidict>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from furl>=2.0.0->clearml->coqui_stt_training) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna->coqui_stt_training) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna->coqui_stt_training) (3.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml->coqui_stt_training) (0.18.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (3.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna->coqui_stt_training) (1.1.3.post0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (1.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (3.4.1)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (2.4.2)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (3.5.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (5.11.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (0.5.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (0.2.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna->coqui_stt_training) (2.0.1)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-1.15.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow 1.15.4\n",
            "Uninstalling tensorflow-1.15.4:\n",
            "  Successfully uninstalled tensorflow-1.15.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (2.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.50.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.13.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.10.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mfatal: destination path 'STT' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "## Install Coqui STT\n",
        "# dependencies\n",
        "! apt-get install sox libsox-fmt-mp3 libopusfile0 libopus-dev libopusfile-dev\n",
        "! pip install --upgrade pip\n",
        "# the Coqui training package\n",
        "! pip install coqui_stt_training\n",
        "! pip uninstall -y tensorflow; pip install \"tensorflow-gpu==1.15\"\n",
        "# code with importer scripts\n",
        "! git clone --depth=1 https://github.com/coqui-ai/STT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Common Voice files to format supported by Coqui\n",
        "! python STT/bin/import_cv_personal.py --normalize metadata.txt clips.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XErZWCwXJe_z",
        "outputId": "1c74a8b4-624b-48a2-c96a-6f90a8c20b67"
      },
      "id": "XErZWCwXJe_z",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TSV file:  /content/metadata.txt\n",
            "Importing mp3 files...\n",
            "WARNING: No --validate_label_locale specified, you might end with inconsistent dataset.\n",
            "WARNING: No --validate_label_locale specified, you might end with inconsistent dataset.\n",
            "Imported 100 samples.\n",
            "Final amount of imported audio: 0:06:35 from 0:06:35.\n",
            "Saving new Coqui STT-formatted CSV file to:  /content/clips/data.csv\n",
            "Writing CSV file for train.py as:  /content/clips/data.csv\n",
            "INFO: compiled /content/data.csv\n",
            "INFO: formatted data located in  /content/clips\n",
            "INFO: you now should decide {train,test,dev} splits on your own\n",
            "INFO: or you can use --auto_input_dataset flag from our training code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove .mp3 files since we already have .wav files (which are the ones\n",
        "# supported by Coqui)\n",
        "\n",
        "# change the name of the folder if it is different (it should be a folder\n",
        "# that contains your extracted audio files, in both .mp3 and .wav format)\n",
        "# name depends on how the audio files are stored inside the zip (they should\n",
        "# be in a folder, ideally with the name \"clips\" to not make any more changes\n",
        "# in code).\n",
        "%cd clips\n",
        "! rm *.mp3\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BvbeGz77IVg",
        "outputId": "11a5343a-869b-485c-9e7e-227576ecfc64"
      },
      "id": "9BvbeGz77IVg",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/clips\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we're going to split the dataset into {train,dev,test}\n",
        "# recommended split: 80/10/10\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/clips/data.csv')\n",
        "\n",
        "df[:80].to_csv('/content/clips/train.csv', index=False)\n",
        "df[80:90].to_csv('/content/clips/dev.csv', index=False)\n",
        "df[90:].to_csv('/content/clips/test.csv', index=False)"
      ],
      "metadata": {
        "id": "_R7jTF7J74sr"
      },
      "id": "_R7jTF7J74sr",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c07a273",
      "metadata": {
        "id": "8c07a273"
      },
      "source": [
        "## âœ… Download pre-trained English model\n",
        "\n",
        "We're going to download a pre-trained STT model for English. This model is the standard Coqui one that you can find in their releases, and with transfer learning we can train a new model which could transcribe any words in any language. In this notebook, we will turn this \"constrained vocabulary\" English model into a more \"open vocabulary\" Romanian model.\n",
        "\n",
        "Coqui STT models as typically stored as checkpoints (for training) and protobufs (for deployment). For transfer learning, we want the **model checkpoints**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "608d203f",
      "metadata": {
        "id": "608d203f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae881e9f-a8da-4340-8cad-78c6ac1cd844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found archive \"english/model.tar.gz\" - not downloading.\n",
            "\n",
            "No extracted pre-trained model found. Extracting now...\n",
            "\n",
            "Model extracted!\n"
          ]
        }
      ],
      "source": [
        "### Download pre-trained model\n",
        "import os\n",
        "import tarfile\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "def download_pretrained_model():\n",
        "    model_dir=\"english/\"\n",
        "    if not os.path.exists(\"english\"):\n",
        "        maybe_download(\"model.tar.gz\", model_dir, \"https://github.com/coqui-ai/STT/releases/download/v1.4.0/coqui-stt-1.4.0-checkpoint.tar.gz\")\n",
        "        print('\\nNo extracted pre-trained model found. Extracting now...')\n",
        "        tar = tarfile.open(\"english/model.tar.gz\")\n",
        "        tar.extractall(\"english/\")\n",
        "        tar.close()\n",
        "        print('\\nModel extracted!')\n",
        "    else:\n",
        "        print('Found \"english/coqui-yesno-checkpoints\" - not extracting.')\n",
        "\n",
        "# Download + extract pre-trained English model\n",
        "download_pretrained_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46b7227",
      "metadata": {
        "id": "b46b7227"
      },
      "source": [
        "## âœ… Configure the training run\n",
        "\n",
        "Coqui STT comes with a long list of hyperparameters you can tweak. We've set default values, but you can use `initialize_globals_from_args()` to set your own. \n",
        "\n",
        "You must **always** configure the paths to your data, and you must **always** configure your alphabet. For transfer learning, it's good practice to define different `load_checkpoint_dir` and `save_checkpoint_dir` paths so that you keep your new model (Romanian STT) separate from the old one (English STT). The parameter `drop_source_layers` allows you to remove layers from the original (aka \"source\") model, and re-initialize them from scratch. If you are fine-tuning to a new alphabet you will have to use _at least_ `drop_source_layers=1` to remove the output layer and add a new output layer which matches your new alphabet.\n",
        "\n",
        "We are fine-tuning a pre-existing model, so `n_hidden` should be the same as the original English model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "cff3c5a0",
      "metadata": {
        "id": "cff3c5a0"
      },
      "outputs": [],
      "source": [
        "! rm -r clips/checkpoints\n",
        "! mkdir clips/checkpoints\n",
        "\n",
        "from coqui_stt_training.util.config import initialize_globals_from_args\n",
        "\n",
        "initialize_globals_from_args(\n",
        "    n_hidden=64,\n",
        "    load_checkpoint_dir=\"english/coqui-yesno-checkpoints\",\n",
        "    save_checkpoint_dir=\"clips/checkpoints\",\n",
        "    drop_source_layers=1,\n",
        "    alphabet_config_path=\"clips/alphabet.txt\",\n",
        "    train_files=[\"clips/train.csv\"],\n",
        "    dev_files=[\"clips/dev.csv\"],\n",
        "    epochs=100,\n",
        "    load_cudnn=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419828c1",
      "metadata": {
        "id": "419828c1"
      },
      "source": [
        "### View all Config settings (*Optional*) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac6ea3d",
      "metadata": {
        "scrolled": true,
        "id": "cac6ea3d"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "print(Config.to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e700d1",
      "metadata": {
        "id": "c8e700d1"
      },
      "source": [
        "## âœ… Train a new Romanian model\n",
        "\n",
        "Let's kick off a training run ðŸš€ðŸš€ðŸš€ (using the configure you set above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8aab2195",
      "metadata": {
        "scrolled": true,
        "id": "8aab2195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2ee354-3920-4b08-fe9e-f5e684b30789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Performing dummy training to check for memory problems.\n",
            "I If the following process crashes, you likely have batch sizes that are too big for your available system memory (or GPU memory).\n",
            "I Could not find best validating checkpoint.\n",
            "I Could not find most recent checkpoint.\n",
            "I Initializing all variables.\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:00 | Steps: 3 | Loss: 599.419983     \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:00 | Steps: 3 | Loss: 433.446218 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:00.771650\n",
            "I Dummy run finished without problems, now starting real training process.\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 227.597880    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 174.940768 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 174.940768 to: clips/checkpoints/best_dev-80\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 130.369687    \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.704137 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.704137 to: clips/checkpoints/best_dev-160\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.637405    \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.770531 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.514668    \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.685316 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.685316 to: clips/checkpoints/best_dev-320\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.449940    \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.600134 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.600134 to: clips/checkpoints/best_dev-400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.405346    \n",
            "Epoch 5 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.553650 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.553650 to: clips/checkpoints/best_dev-480\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.321855    \n",
            "Epoch 6 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.550535 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.550535 to: clips/checkpoints/best_dev-560\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.304644    \n",
            "Epoch 7 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.466449 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.466449 to: clips/checkpoints/best_dev-640\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.194610    \n",
            "Epoch 8 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.428526 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.428526 to: clips/checkpoints/best_dev-720\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.150695    \n",
            "Epoch 9 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.376160 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.376160 to: clips/checkpoints/best_dev-800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.133634   \n",
            "Epoch 10 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.339954 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.339954 to: clips/checkpoints/best_dev-880\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 120.125336   \n",
            "Epoch 11 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.314586 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.314586 to: clips/checkpoints/best_dev-960\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 119.992093   \n",
            "Epoch 12 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.221170 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.221170 to: clips/checkpoints/best_dev-1040\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 119.912631   \n",
            "Epoch 13 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.097287 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.097287 to: clips/checkpoints/best_dev-1120\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 119.818882   \n",
            "Epoch 14 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.036102 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 109.036102 to: clips/checkpoints/best_dev-1200\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 119.245990   \n",
            "Epoch 15 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 108.096721 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 108.096721 to: clips/checkpoints/best_dev-1280\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 118.326890   \n",
            "Epoch 16 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 106.361506 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 106.361506 to: clips/checkpoints/best_dev-1360\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 115.935151   \n",
            "Epoch 17 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 104.595011 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 104.595011 to: clips/checkpoints/best_dev-1440\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 113.128106   \n",
            "Epoch 18 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 102.386282 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 102.386282 to: clips/checkpoints/best_dev-1520\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 110.119124   \n",
            "Epoch 19 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 100.322177 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 100.322177 to: clips/checkpoints/best_dev-1600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 107.004316   \n",
            "Epoch 20 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 97.116399 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 97.116399 to: clips/checkpoints/best_dev-1680\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 103.754656   \n",
            "Epoch 21 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 95.496769 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 95.496769 to: clips/checkpoints/best_dev-1760\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 100.091378   \n",
            "Epoch 22 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 92.745417 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 92.745417 to: clips/checkpoints/best_dev-1840\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 95.534171    \n",
            "Epoch 23 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 88.993054 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 88.993054 to: clips/checkpoints/best_dev-1920\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 91.331622    \n",
            "Epoch 24 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 87.099405 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 87.099405 to: clips/checkpoints/best_dev-2000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 87.224446    \n",
            "Epoch 25 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 84.747620 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 84.747620 to: clips/checkpoints/best_dev-2080\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 83.484456    \n",
            "Epoch 26 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 83.391504 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 83.391504 to: clips/checkpoints/best_dev-2160\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 78.524166    \n",
            "Epoch 27 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 83.654217 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 75.943046    \n",
            "Epoch 28 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 80.316056 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 80.316056 to: clips/checkpoints/best_dev-2320\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 71.993250    \n",
            "Epoch 29 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 80.398080 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 68.589836    \n",
            "Epoch 30 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 80.450541 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 64.707101    \n",
            "Epoch 31 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 79.117463 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 79.117463 to: clips/checkpoints/best_dev-2560\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 61.304532    \n",
            "Epoch 32 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 77.250364 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 77.250364 to: clips/checkpoints/best_dev-2640\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 58.469776    \n",
            "Epoch 33 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 78.939708 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 56.604173    \n",
            "Epoch 34 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 76.286812 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 76.286812 to: clips/checkpoints/best_dev-2800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 53.618984    \n",
            "Epoch 35 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 75.334683 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 75.334683 to: clips/checkpoints/best_dev-2880\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 52.858394    \n",
            "Epoch 36 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 77.022033 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 51.018832    \n",
            "Epoch 37 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 77.746382 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 47.163812    \n",
            "Epoch 38 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 75.659212 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 45.335887    \n",
            "Epoch 39 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 77.735774 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 43.937139    \n",
            "Epoch 40 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 75.661052 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 43.426076    \n",
            "Epoch 41 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 79.593839 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 42.152200    \n",
            "Epoch 42 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 77.052895 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 39.818839    \n",
            "Epoch 43 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 83.960817 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 38.871795    \n",
            "Epoch 44 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 82.355270 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 37.879813    \n",
            "Epoch 45 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 82.896753 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 36.223139    \n",
            "Epoch 46 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 83.787194 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 34.389981    \n",
            "Epoch 47 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 85.218525 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 33.165474    \n",
            "Epoch 48 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 86.378929 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 32.667691    \n",
            "Epoch 49 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 84.852564 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 33.092167    \n",
            "Epoch 50 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 80.233767 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 51 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 29.023354    \n",
            "Epoch 51 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 80.522267 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 52 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 27.966970    \n",
            "Epoch 52 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 89.107984 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 53 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 28.160662    \n",
            "Epoch 53 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 86.389143 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 54 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 26.446785    \n",
            "Epoch 54 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 91.262617 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 55 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 25.412421    \n",
            "Epoch 55 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 94.093310 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 56 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 24.208373    \n",
            "Epoch 56 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 91.404432 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 57 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 24.887650    \n",
            "Epoch 57 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 91.593285 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 58 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 23.612843    \n",
            "Epoch 58 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 95.698381 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 59 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 24.221754    \n",
            "Epoch 59 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 94.107007 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 60 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 22.272878    \n",
            "Epoch 60 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 97.492791 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 61 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 20.886942    \n",
            "Epoch 61 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 100.392313 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 62 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 20.938799    \n",
            "Epoch 62 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 96.489038 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 63 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 19.328007    \n",
            "Epoch 63 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 88.046187 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 64 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 19.382538    \n",
            "Epoch 64 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 93.553191 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 65 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 18.913358    \n",
            "Epoch 65 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 100.495261 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 66 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 17.503529    \n",
            "Epoch 66 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 95.010467 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 67 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 18.008265    \n",
            "Epoch 67 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 115.345382 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 68 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 17.370252    \n",
            "Epoch 68 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 102.984732 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 69 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 18.286987    \n",
            "Epoch 69 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 118.547977 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 70 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 17.358069    \n",
            "Epoch 70 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 103.237754 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 71 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 16.011132    \n",
            "Epoch 71 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 100.760637 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 72 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 15.142522    \n",
            "Epoch 72 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 97.034166 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 73 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.079573    \n",
            "Epoch 73 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 99.993582 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 74 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 14.173050    \n",
            "Epoch 74 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 103.578552 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 75 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.388541    \n",
            "Epoch 75 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 104.161018 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 76 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.356571    \n",
            "Epoch 76 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 107.943887 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 77 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.484028    \n",
            "Epoch 77 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 105.139055 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 78 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.245291    \n",
            "Epoch 78 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 112.361323 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 79 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 13.117720    \n",
            "Epoch 79 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 108.569632 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 80 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 12.752198    \n",
            "Epoch 80 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 109.466825 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 81 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 11.703835    \n",
            "Epoch 81 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 108.875118 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 82 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 11.520012    \n",
            "Epoch 82 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 113.443142 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 83 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 12.762993    \n",
            "Epoch 83 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 107.426314 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 84 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 12.149733    \n",
            "Epoch 84 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 112.768331 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 85 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 10.699900    \n",
            "Epoch 85 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 115.807540 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 86 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 11.002461    \n",
            "Epoch 86 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 102.584710 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 87 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 10.436804    \n",
            "Epoch 87 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 105.725503 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 88 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 10.945333    \n",
            "Epoch 88 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 107.423106 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 89 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 9.940712     \n",
            "Epoch 89 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 114.265703 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 90 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 11.059646    \n",
            "Epoch 90 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 111.938985 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 91 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 9.593025     \n",
            "Epoch 91 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 111.565950 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 92 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 10.180209    \n",
            "Epoch 92 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 113.334886 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 93 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 11.077726    \n",
            "Epoch 93 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 114.921576 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 94 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 9.300907     \n",
            "Epoch 94 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 114.260857 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 95 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 9.440220     \n",
            "Epoch 95 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 114.787622 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 96 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 8.697207     \n",
            "Epoch 96 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 118.733265 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 97 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 8.308853     \n",
            "Epoch 97 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 117.033883 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 98 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 7.937919     \n",
            "Epoch 98 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 114.292223 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 99 |   Training | Elapsed Time: 0:00:04 | Steps: 80 | Loss: 8.796682     \n",
            "Epoch 99 | Validation | Elapsed Time: 0:00:00 | Steps: 10 | Loss: 128.247672 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:07:59.901711\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.train import train\n",
        "\n",
        "# use maximum one GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c87ba61",
      "metadata": {
        "id": "3c87ba61"
      },
      "source": [
        "## âœ… Configure the testing run\n",
        "\n",
        "Let's add the path to our testing data and update `load_checkpoint_dir` to our new model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "2be7beb5",
      "metadata": {
        "id": "2be7beb5"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "Config.test_files=[\"clips/test.csv\"]\n",
        "Config.load_checkpoint_dir=\"clips/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a5c971",
      "metadata": {
        "id": "c6a5c971"
      },
      "source": [
        "## âœ… Test the new Romanian model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6222dc69",
      "metadata": {
        "id": "6222dc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94acf04-17a6-46fa-9bfd-6eafe848fca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Loading best validating checkpoint from clips/checkpoints/best_dev-2880\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on clips/test.csv\n",
            "Test epoch | Steps: 10 | Elapsed Time: 0:00:04                                 \n",
            "Test on clips/test.csv - WER: 0.973684, CER: 0.483721, loss: 92.980423\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.833333, CER: 0.315789, loss: 60.865948\n",
            " - wav: clips/9cafe5e30dfbf3d273f46c734606bac8fa21a5ec53787549f4f52a1a8048eabe.wav\n",
            " - src: \"aceasta este baza dezbaterii de astazi\"\n",
            " - res: \"aceasta esteasa deateieasta\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.857143, CER: 0.530612, loss: 78.711533\n",
            " - wav: clips/9caecc1b024d0851eaaea51ab460c3135d1d25ba179e2dc39fa504a712b12ecd.wav\n",
            " - src: \"consecintele testelor trebuie sa fie foarte clare\"\n",
            " - res: \"ose titetetaoare sa tiecotetoare\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.875000, CER: 0.564103, loss: 80.586510\n",
            " - wav: clips/9c8cf0d391b1631303503c019405e5d537d70cac8b38fd34f334aae369a35d95.wav\n",
            " - src: \"sper ca votul va avea loc foarte curand\"\n",
            " - res: \"sper coooadalaccate cpr\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.923077, CER: 0.430769, loss: 117.283379\n",
            " - wav: clips/9ca96d025cedc64b52f0b5a8ad8f931af93c6aa68c8b88487bb95bfb703070e8.wav\n",
            " - src: \"speranta te face sa traiesti dar ca pe o funie intinsa peste abis\"\n",
            " - res: \"speata pe pacesantariesar caaoucuentisaestaris\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.547619, loss: 128.265335\n",
            " - wav: clips/9cb0e471bb573e8dd58fe66c9905df3c1699246cd160c5726d0fe6afd22ac55a.wav\n",
            " - src: \"cinci mii patru sute saizeci si sapte euro\"\n",
            " - res: \"csiia usatesee sisateeera\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.923077, CER: 0.430769, loss: 117.283379\n",
            " - wav: clips/9ca96d025cedc64b52f0b5a8ad8f931af93c6aa68c8b88487bb95bfb703070e8.wav\n",
            " - src: \"speranta te face sa traiesti dar ca pe o funie intinsa peste abis\"\n",
            " - res: \"speata pe pacesantariesar caaoucuentisaestaris\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.547619, loss: 128.265335\n",
            " - wav: clips/9cb0e471bb573e8dd58fe66c9905df3c1699246cd160c5726d0fe6afd22ac55a.wav\n",
            " - src: \"cinci mii patru sute saizeci si sapte euro\"\n",
            " - res: \"csiia usatesee sisateeera\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.523810, loss: 96.039604\n",
            " - wav: clips/9c89d067e435c60121e19750fd6f00f6dff196e1d8e87a428f17b2d983039680.wav\n",
            " - src: \"inovatia este desigur extrem de importanta\"\n",
            " - res: \"ioaese esinoesediei pota ta\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.500000, loss: 84.449799\n",
            " - wav: clips/9c8ff45b438e422ac0d92f90b2b9d205714506e03d2e9beff4f6808701e5c92e.wav\n",
            " - src: \"ea este o expresie a solidaritatii\"\n",
            " - res: \"iea esae tsresea sari raritec\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.600000, loss: 82.595985\n",
            " - wav: clips/9c95cbadca755520211947b3c8a0761e9daefb15e7afbb52a277a0ad1669bb82.wav\n",
            " - src: \"szekely douazeci si cinci\"\n",
            " - res: \"serereooesicii\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.523810, loss: 96.039604\n",
            " - wav: clips/9c89d067e435c60121e19750fd6f00f6dff196e1d8e87a428f17b2d983039680.wav\n",
            " - src: \"inovatia este desigur extrem de importanta\"\n",
            " - res: \"ioaese esinoesediei pota ta\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.500000, loss: 84.449799\n",
            " - wav: clips/9c8ff45b438e422ac0d92f90b2b9d205714506e03d2e9beff4f6808701e5c92e.wav\n",
            " - src: \"ea este o expresie a solidaritatii\"\n",
            " - res: \"iea esae tsresea sari raritec\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.600000, loss: 82.595985\n",
            " - wav: clips/9c95cbadca755520211947b3c8a0761e9daefb15e7afbb52a277a0ad1669bb82.wav\n",
            " - src: \"szekely douazeci si cinci\"\n",
            " - res: \"serereooesicii\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.431818, loss: 72.312653\n",
            " - wav: clips/9ca2478ec2fecb9e0c6e9a4c207575c96b7f7fec7935d911248981741b39ce64.wav\n",
            " - src: \"ne doare pe toti inca de a doua zi dimineata\"\n",
            " - res: \"earea peta tsipinca dea daziinieaa\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.250000, CER: 0.461538, loss: 128.693481\n",
            " - wav: clips/9c92f91f5b112447bfd8945f16032aafcb4e5317b312731373249c4afd92c016.wav\n",
            " - src: \"cu toate acestea este si multumita presedintiei cehe\"\n",
            " - res: \"ua te acestea esese u si ta e si i siice e\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.evaluate import test\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXM2vxfJBa_E"
      },
      "id": "PXM2vxfJBa_E",
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}