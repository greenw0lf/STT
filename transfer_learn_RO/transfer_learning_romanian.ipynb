{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45ea3ef5",
      "metadata": {
        "id": "45ea3ef5"
      },
      "source": [
        "# Transfer learn from English to Romanian STT model\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Setup the Romanian audio and metadata files.\n",
        "2. Download a pre-trained English STT model.\n",
        "3. Fine-tune the English model to Romanian language.\n",
        "4. Test the new Romanian model and display its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2aec77",
      "metadata": {
        "id": "fa2aec77"
      },
      "outputs": [],
      "source": [
        "## Install Coqui STT\n",
        "# dependencies\n",
        "! apt-get install sox libsox-fmt-mp3 libopusfile0 libopus-dev libopusfile-dev\n",
        "! pip install --upgrade pip\n",
        "# the Coqui training package\n",
        "! pip install coqui_stt_training\n",
        "! pip install \"tensorflow-gpu==1.15\"\n",
        "# code with importer scripts\n",
        "! git clone --depth=1 https://github.com/coqui-ai/STT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metadata preprocessing\n",
        "We need to preprocess the metadata file a bit. This includes removing punctuations, making text be all lowercase, and replacing some diacritics that were wrongly annotated with their correct versions.\n",
        "</br>\n",
        "</br>\n",
        "We want to make the model as easy as possible to train, hence why we exclude uppercase characters and punctuations."
      ],
      "metadata": {
        "id": "eZNeWidINK2y"
      },
      "id": "eZNeWidINK2y"
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuations and make all letters lowercase\n",
        "\n",
        "data = ''\n",
        "with open('metadata.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "    for char in ['.', '?', '!', '\"', '”', ',']:\n",
        "      data = data.replace(char, '')\n",
        "    # replace wrong diacritics with the proper ones\n",
        "    data = data.replace('ş', 'ș')\n",
        "    data = data.replace('ţ', 'ț')\n",
        "    data = data.lower()\n",
        "with open('metadata.txt', 'w') as file:\n",
        "    file.write(data)"
      ],
      "metadata": {
        "id": "tlBTkFevDQVi"
      },
      "id": "tlBTkFevDQVi",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert files to Coqui STT compatible format"
      ],
      "metadata": {
        "id": "LT7mIoaKN1LV"
      },
      "id": "LT7mIoaKN1LV"
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Common Voice files to format supported by Coqui\n",
        "! python STT/bin/import_cv_personal.py metadata.txt clips.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XErZWCwXJe_z",
        "outputId": "f73d15ae-df3f-4063-f2a8-aa9979c0dcdb"
      },
      "id": "XErZWCwXJe_z",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TSV file:  /content/metadata.txt\n",
            "Importing mp3 files...\n",
            "WARNING: No --validate_label_locale specified, you might end with inconsistent dataset.\n",
            "WARNING: No --validate_label_locale specified, you might end with inconsistent dataset.\n",
            "Imported 60 samples.\n",
            "Final amount of imported audio: 0:03:56 from 0:03:56.\n",
            "Saving new Coqui STT-formatted CSV file to:  /content/clips/data.csv\n",
            "Writing CSV file for train.py as:  /content/clips/data.csv\n",
            "INFO: compiled /content/data.csv\n",
            "INFO: formatted data located in  /content/clips\n",
            "INFO: you now should decide {train,test,dev} splits on your own\n",
            "INFO: or you can use --auto_input_dataset flag from our training code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove .mp3 files since we already have .wav files (which are the ones\n",
        "# supported by Coqui)\n",
        "\n",
        "# change the name of the folder if it is different (it should be a folder\n",
        "# that contains your extracted audio files, in both .mp3 and .wav format)\n",
        "# name depends on how the audio files are stored inside the zip (they should\n",
        "# be in a folder, ideally with the name \"clips\" to not make any more changes\n",
        "# in code).\n",
        "%cd clips\n",
        "! rm *.mp3\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BvbeGz77IVg",
        "outputId": "68b16f50-adae-4b47-dbd9-6dbb04055baa"
      },
      "id": "9BvbeGz77IVg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/clips\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data split\n",
        "\n",
        "We need to split the data into a training set, a validation(dev) set, and a test set. I decided to use the ratio 80/10/10 for the distribution."
      ],
      "metadata": {
        "id": "encwlx9aN9H9"
      },
      "id": "encwlx9aN9H9"
    },
    {
      "cell_type": "code",
      "source": [
        "# now we're going to split the dataset into {train,dev,test}\n",
        "# recommended split: 80/10/10 (in our case, 60 files total\n",
        "# so 50/5/5)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/clips/data.csv')\n",
        "\n",
        "df[:50].to_csv('/content/clips/train.csv', index=False)\n",
        "df[50:55].to_csv('/content/clips/dev.csv', index=False)\n",
        "df[55:].to_csv('/content/clips/test.csv', index=False)"
      ],
      "metadata": {
        "id": "_R7jTF7J74sr"
      },
      "id": "_R7jTF7J74sr",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c07a273",
      "metadata": {
        "id": "8c07a273"
      },
      "source": [
        "## ✅ Download pre-trained English model\n",
        "\n",
        "We're going to download a pre-trained STT model for English. This model is the standard Coqui one that you can find in their releases, and with transfer learning we can train a new model which could transcribe any words in any language. In this notebook, we will turn this \"constrained vocabulary\" English model into a more \"open vocabulary\" Romanian model.\n",
        "\n",
        "Coqui STT models as typically stored as checkpoints (for training) and protobufs (for deployment). For transfer learning, we want the **model checkpoints**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "608d203f",
      "metadata": {
        "id": "608d203f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35958b4-148d-45a8-c785-3d67409d89b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No path \"english/\" - creating ...\n",
            "No archive \"english/model.tar.gz\" - downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 647315187/647315187 [00:07<00:00, 83301228.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No extracted pre-trained model found. Extracting now...\n",
            "\n",
            "Model extracted!\n"
          ]
        }
      ],
      "source": [
        "### Download pre-trained model\n",
        "import os\n",
        "import tarfile\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "def download_pretrained_model():\n",
        "    model_dir=\"english/\"\n",
        "    if not os.path.exists(\"english\"):\n",
        "        maybe_download(\"model.tar.gz\", model_dir, \"https://github.com/coqui-ai/STT/releases/download/v1.4.0/coqui-stt-1.4.0-checkpoint.tar.gz\")\n",
        "        print('\\nNo extracted pre-trained model found. Extracting now...')\n",
        "        tar = tarfile.open(\"english/model.tar.gz\")\n",
        "        tar.extractall(\"english/\")\n",
        "        tar.close()\n",
        "        print('\\nModel extracted!')\n",
        "    else:\n",
        "        print('Found \"english/coqui-yesno-checkpoints\" - not extracting.')\n",
        "\n",
        "# Download + extract pre-trained English model\n",
        "download_pretrained_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case there is a previous run done, run this to clean the model\n",
        "! rm -r clips/checkpoints\n",
        "# create the checkpoints dir to store and test new model\n",
        "! mkdir clips/checkpoints\n",
        "# move the alphabet to clips folder (consistent with other data files related \n",
        "# to Romanian)\n",
        "! cp alphabet.txt clips/alphabet.txt"
      ],
      "metadata": {
        "id": "qFx4zQiZMs0m"
      },
      "id": "qFx4zQiZMs0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b46b7227",
      "metadata": {
        "id": "b46b7227"
      },
      "source": [
        "## ✅ Configure the training run\n",
        "\n",
        "Coqui STT comes with a long list of hyperparameters you can tweak. We've set default values, but you can use `initialize_globals_from_args()` to set your own. \n",
        "\n",
        "You must **always** configure the paths to your data, and you must **always** configure your alphabet. For transfer learning, it's good practice to define different `load_checkpoint_dir` and `save_checkpoint_dir` paths so that you keep your new model (Romanian STT) separate from the old one (English STT). The parameter `drop_source_layers` allows you to remove layers from the original (aka \"source\") model, and re-initialize them from scratch. If you are fine-tuning to a new alphabet you will have to use _at least_ `drop_source_layers=1` to remove the output layer and add a new output layer which matches your new alphabet.\n",
        "\n",
        "We are fine-tuning a pre-existing model, so `n_hidden` should be the same as the original English model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cff3c5a0",
      "metadata": {
        "id": "cff3c5a0"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import initialize_globals_from_args\n",
        "\n",
        "initialize_globals_from_args(\n",
        "    n_hidden=2048,\n",
        "    load_checkpoint_dir=\"english/coqui-stt-1.4.0-checkpoint\",\n",
        "    save_checkpoint_dir=\"clips/checkpoints\",\n",
        "    drop_source_layers=1,\n",
        "    alphabet_config_path=\"clips/alphabet.txt\",\n",
        "    train_files=[\"clips/train.csv\"],\n",
        "    dev_files=[\"clips/dev.csv\"],\n",
        "    epochs=5,\n",
        "    load_cudnn=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e700d1",
      "metadata": {
        "id": "c8e700d1"
      },
      "source": [
        "## ✅ Train a new Romanian model\n",
        "\n",
        "Let's kick off a training run 🚀🚀🚀 (using the configure you set above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8aab2195",
      "metadata": {
        "scrolled": true,
        "id": "8aab2195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa782d3-231e-45a3-b19b-5eb48ea81b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Performing dummy training to check for memory problems.\n",
            "I If the following process crashes, you likely have batch sizes that are too big for your available system memory (or GPU memory).\n",
            "I Loading best validating checkpoint from english/coqui-stt-1.4.0-checkpoint/best_dev-3663881\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:02:12 | Steps: 3 | Loss: 602.980326     \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:07 | Steps: 3 | Loss: 597.148407 | Dataset: clips/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:02:19.523779\n",
            "I Dummy run finished without problems, now starting real training process.\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:28:53 | Steps: 50 | Loss: 354.439228    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:11 | Steps: 5 | Loss: 227.644385 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 227.644385 to: clips/checkpoints/best_dev-3663931\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:29:26 | Steps: 50 | Loss: 216.316908    \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:11 | Steps: 5 | Loss: 190.872577 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 190.872577 to: clips/checkpoints/best_dev-3663981\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:28:03 | Steps: 50 | Loss: 189.135495    \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:10 | Steps: 5 | Loss: 180.381595 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 180.381595 to: clips/checkpoints/best_dev-3664031\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:28:47 | Steps: 50 | Loss: 176.364152    \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:11 | Steps: 5 | Loss: 178.517636 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 178.517636 to: clips/checkpoints/best_dev-3664081\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:27:45 | Steps: 50 | Loss: 169.248023    \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:11 | Steps: 5 | Loss: 176.948236 | Dataset: clips/dev.csv\n",
            "I Saved new best validating model with loss 176.948236 to: clips/checkpoints/best_dev-3664131\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 2:24:10.189137\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.train import train\n",
        "\n",
        "# use maximum one GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c87ba61",
      "metadata": {
        "id": "3c87ba61"
      },
      "source": [
        "## ✅ Configure the testing run\n",
        "\n",
        "Let's add the path to our testing data and update `load_checkpoint_dir` to our new model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2be7beb5",
      "metadata": {
        "id": "2be7beb5"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "Config.test_files=[\"clips/test.csv\"]\n",
        "Config.load_checkpoint_dir=\"clips/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a5c971",
      "metadata": {
        "id": "c6a5c971"
      },
      "source": [
        "## ✅ Test the new Romanian model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6222dc69",
      "metadata": {
        "id": "6222dc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423dc22a-b435-4de6-adc6-137b82af5ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Loading best validating checkpoint from clips/checkpoints/best_dev-3664131\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on clips/test.csv\n",
            "Test epoch | Steps: 5 | Elapsed Time: 0:00:15                                  \n",
            "Test on clips/test.csv - WER: 1.000000, CER: 0.727700, loss: 289.216064\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.125000, CER: 0.756098, loss: 139.401489\n",
            " - wav: clips/9bf530bb72564728291fb50f9e887bd8ad2d3ac25009ff15ed1e157eedbbdfd6.wav\n",
            " - src: \"după cum am spus speranța de viață crește\"\n",
            " - res: \"te e e e a r r r rae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.142857, CER: 0.760000, loss: 184.467361\n",
            " - wav: clips/9bcb84baf6a487bb3bd4bd4b01ce8af064927592647d250ef6c9e7d7f43d5010.wav\n",
            " - src: \"așadar această dezbatere este una foarte personală\"\n",
            " - res: \"te e e e r r r iae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.375000, CER: 0.685714, loss: 154.263367\n",
            " - wav: clips/9bd4f142f61610e8da0793f14532858d3f5c980c89a4e0e540d9a0849dbd0913.wav\n",
            " - src: \"toți trebuie să simtă că au o șansă\"\n",
            " - res: \"te e e a a a r r r r taeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.857143, CER: 0.659091, loss: 128.772507\n",
            " - wav: clips/9bc7b5a21567e615734594d79473f84c7f0a287a6e6590b949ecb0d7239dbc76.wav\n",
            " - src: \"cred că aceste temeri sunt complet nefondate\"\n",
            " - res: \"te e e e e r r r r r e e aeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 2.000000, CER: 0.767442, loss: 839.175659\n",
            " - wav: clips/9bd646b3be1b72ab8b555358c54e0437d15a28dfeeaadee431e0958e15a18f7d.wav\n",
            " - src: \"nouă misiuni și au încetat deja activitatea\"\n",
            " - res: \"te e e e a a r r r r r r r aiae\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.125000, CER: 0.756098, loss: 139.401489\n",
            " - wav: clips/9bf530bb72564728291fb50f9e887bd8ad2d3ac25009ff15ed1e157eedbbdfd6.wav\n",
            " - src: \"după cum am spus speranța de viață crește\"\n",
            " - res: \"te e e e a r r r rae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.142857, CER: 0.760000, loss: 184.467361\n",
            " - wav: clips/9bcb84baf6a487bb3bd4bd4b01ce8af064927592647d250ef6c9e7d7f43d5010.wav\n",
            " - src: \"așadar această dezbatere este una foarte personală\"\n",
            " - res: \"te e e e r r r iae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.375000, CER: 0.685714, loss: 154.263367\n",
            " - wav: clips/9bd4f142f61610e8da0793f14532858d3f5c980c89a4e0e540d9a0849dbd0913.wav\n",
            " - src: \"toți trebuie să simtă că au o șansă\"\n",
            " - res: \"te e e a a a r r r r taeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.857143, CER: 0.659091, loss: 128.772507\n",
            " - wav: clips/9bc7b5a21567e615734594d79473f84c7f0a287a6e6590b949ecb0d7239dbc76.wav\n",
            " - src: \"cred că aceste temeri sunt complet nefondate\"\n",
            " - res: \"te e e e e r r r r r e e aeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 2.000000, CER: 0.767442, loss: 839.175659\n",
            " - wav: clips/9bd646b3be1b72ab8b555358c54e0437d15a28dfeeaadee431e0958e15a18f7d.wav\n",
            " - src: \"nouă misiuni și au încetat deja activitatea\"\n",
            " - res: \"te e e e a a r r r r r r r aiae\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.125000, CER: 0.756098, loss: 139.401489\n",
            " - wav: clips/9bf530bb72564728291fb50f9e887bd8ad2d3ac25009ff15ed1e157eedbbdfd6.wav\n",
            " - src: \"după cum am spus speranța de viață crește\"\n",
            " - res: \"te e e e a r r r rae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.142857, CER: 0.760000, loss: 184.467361\n",
            " - wav: clips/9bcb84baf6a487bb3bd4bd4b01ce8af064927592647d250ef6c9e7d7f43d5010.wav\n",
            " - src: \"așadar această dezbatere este una foarte personală\"\n",
            " - res: \"te e e e r r r iae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.375000, CER: 0.685714, loss: 154.263367\n",
            " - wav: clips/9bd4f142f61610e8da0793f14532858d3f5c980c89a4e0e540d9a0849dbd0913.wav\n",
            " - src: \"toți trebuie să simtă că au o șansă\"\n",
            " - res: \"te e e a a a r r r r taeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.857143, CER: 0.659091, loss: 128.772507\n",
            " - wav: clips/9bc7b5a21567e615734594d79473f84c7f0a287a6e6590b949ecb0d7239dbc76.wav\n",
            " - src: \"cred că aceste temeri sunt complet nefondate\"\n",
            " - res: \"te e e e e r r r r r e e aeae\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 2.000000, CER: 0.767442, loss: 839.175659\n",
            " - wav: clips/9bd646b3be1b72ab8b555358c54e0437d15a28dfeeaadee431e0958e15a18f7d.wav\n",
            " - src: \"nouă misiuni și au încetat deja activitatea\"\n",
            " - res: \"te e e e a a r r r r r r r aiae\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.evaluate import test\n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
